{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Copy of Wine Testing Final Project (1).ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DariaEng2704/Final-Project/blob/main/Wine_Testing_Final_Project_Final_try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdaFGHD-xvfC"
      },
      "source": [
        "# ***Quality of Wine - She Codes Final Project***\n",
        "\n",
        "\n",
        "\n",
        "by Daria Engel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybcxsSUdxvfI"
      },
      "source": [
        "The current project will provide supervised predictive models to identify the quality of wines, based on a variety of parameters, as country of origin, price, and variety of the wine. \n",
        "The provided models will help us to predict the quality of a wine, and therefore, help us to understand is it worth buying. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WIvX1pLxvfK"
      },
      "source": [
        "# **Import the modules**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNNPAqzTxvfK"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from datetime import datetime\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow.keras\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import statsmodels.api as sm\n",
        "from termcolor import colored as cl\n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.metrics import plot_confusion_matrix \n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR01O4o7xvfL"
      },
      "source": [
        "\n",
        "\n",
        "# **Import the data**\n",
        "\n",
        "\n",
        "We will load in a dataset from the **[Kaggle website](https://www.kaggle.com/)**.\n",
        "Specifically, we are going to use the **[Wine Reviews Dataset](https://www.kaggle.com/zynicide/wine-reviews?select=winemag-data-130k-v2.csv)**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz8rvE-gxvfL"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "data = pd.read_csv('winemag-data-130k-v2.csv')\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Preview of the first 5 rows of the data:\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCRSlYqoxvfM"
      },
      "source": [
        "# We will remove the first column, which contains the numbering of the rows.\n",
        "\n",
        "df = df.drop(df.columns[0], axis=1) \n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwxlhV-oxvfM"
      },
      "source": [
        "The columns included in the data are : \n",
        "\n",
        "+ **country** - the country the wine comes from.\n",
        "+ **description** - the description of the wine by the taster.\n",
        "+ **designation** - the vineyard within the winery provided the grapes for the wine.\n",
        "+ **points** - the number of points the taster rated the wine on a scale of 1-100 (the current dataset includes only a range of 80-100 as the website which provided the datadset posts those scores only. \n",
        "+ **price** - the cost of a bottle of wine.\n",
        "+ **province** - the province or state the wine comes from.\n",
        "+ **region_1** - the wine-growing area in a province or state.\n",
        "+ **region_2** - specific regions specified within a wine-growing area.\n",
        "+ **taster_name** - the name of the taster of the wine. \n",
        "+ **taster_twitter_handle** - the Twitter username of the taster.\n",
        "+ **title** - the title of the wine.\n",
        "+ **variety** - the variety of the wine.\n",
        "+ **winery** - the winery the wine was made by. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SakmfSqNxvfM"
      },
      "source": [
        "# **Identifying Missing Data**\n",
        "\n",
        "In the current section, we will focus on identifying the missing data in the dataset. \n",
        "First, we will check the type of data in each column :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-s0SAZbxvfN"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgpfBmbOxvfN"
      },
      "source": [
        "After checking that all the suggested data types are matching the real data types, we will drop the duplicate samples -  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfbD5J5FxvfN"
      },
      "source": [
        "print (\"Data size prior duplicates removal: \" + str(len(df.axes[0])) + \" rows and \" + str(len(df.axes[1])) + \" columns.\")\n",
        "df = df.drop_duplicates()\n",
        "print (\"Data size after duplicates removal: \" + str(len(df.axes[0])) + \" rows and \" + str(len(df.axes[1])) + \" columns.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4BpKxtXefG5"
      },
      "source": [
        "Now, we will make a summary table with  \n",
        " + Features with missing data\n",
        " + The percent of the missing data for each feature\n",
        " + How many unique values each feature have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IprLVxRaxvfO"
      },
      "source": [
        "nulls = df.isnull().sum()\n",
        "percentage_nulls = 100 * nulls / len(df)\n",
        "data_types = df.dtypes\n",
        "unique_values = df.nunique()\n",
        "missing_values_table = pd.concat([nulls, percentage_nulls,unique_values, data_types], axis=1)\n",
        "missing_values_table = missing_values_table.rename(columns = {0 : 'Missing Values', 1 : 'Percentage', 2: 'Unique Values', 3 : 'Data Types'})\n",
        "missing_values_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB0pqvmzxvfO"
      },
      "source": [
        "# **Missing Data Imputation**\n",
        "\n",
        "#### First, we will start with **observations** dropping : \n",
        "  + The features **\"country\"**, **\"province\"** and **\"variety\"** have a negligible amount of missing values. Therefore, we will remove the samples with those values : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd2Zsd2uxvfO"
      },
      "source": [
        "df_no_small_missing = df[(df['country'].notnull())  & (df['variety'].notnull()) & (df['province'].notnull())]\n",
        "df_no_small_missing.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG0IQFWpxvfO"
      },
      "source": [
        "#### Secondly, we will drop some **features** : \n",
        "\n",
        "+ The **\"designation\"** variable will not be used since 30% of the data is missing and the rest 70% includes many unique values (every second value is unique), which makes it not very useful to use. \n",
        "+ The **\"region_2\"** variable has a very high percentage of missing data (61%), so we will drop it. This variable only contains regions in the USA and irrelevant to all other countries (see below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdMPknt4xvfP"
      },
      "source": [
        "df_no_small_missing['region_2'].value_counts().plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQibYjZXxvfP"
      },
      "source": [
        "+ For now, we will also drop the **\"description\"** variable, since it requires an NLP model. \n",
        "+ The variables **\"title\"** and **\"taster_twitter_handle\"** will be dropped. \n",
        "  + \"title\" data is very specific and unique, which is not informative for the models. \n",
        "  + \"taster name\" is covered by the \"taster_name\" variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQEq-Ga6xvfP"
      },
      "source": [
        "# Drop of all mentioned above columns \n",
        "\n",
        "df_relevant_features = df_no_small_missing.drop(['description','designation','region_2','title','taster_twitter_handle'], axis=1) \n",
        "df_relevant_features.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXpDwzIdxvfP"
      },
      "source": [
        "nulls = df_relevant_features.isnull().sum()\n",
        "percentage_nulls = 100 * nulls / len(df_relevant_features)\n",
        "data_types = df_relevant_features.dtypes\n",
        "unique_values = df_relevant_features.nunique()\n",
        "missing_values_table = pd.concat([nulls, percentage_nulls,unique_values, data_types], axis=1)\n",
        "missing_values_table = missing_values_table.rename(columns = {0 : 'Missing Values', 1 : 'Percentage', 2: 'Unique Values', 3 : 'Data Types'})\n",
        "missing_values_table"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgHPykJ2xvfQ"
      },
      "source": [
        "## **Missing Data Imputaion - \"price\" variable** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hRGQwCWxvfQ"
      },
      "source": [
        "We will start by imputing the missing data in the \"price\" variable (integer). The missing values will be replaced with a median of a price in each province, with a belief that these groups are homogeneous enough to obtain relatively accurate results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nFFIzXIxvfQ"
      },
      "source": [
        "df_relevant_features['price'] = df_relevant_features.groupby('province')['price'].transform(lambda x: x.fillna(x.median()))\n",
        "print(df_relevant_features.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIgwwye_xvfQ"
      },
      "source": [
        "We left with 3 missing values for the \"price\" variable. As possible to see below, all three have only one value per province and this value is missing. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1NhXNh1xvfQ"
      },
      "source": [
        "print (df_relevant_features.loc[pd.isna(df_relevant_features['price'])]['province'].value_counts())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldg99MG4xvfR"
      },
      "source": [
        "df_relevant_features.loc[pd.isna(df_relevant_features['price'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tL5iSuPQxvfR"
      },
      "source": [
        "Therefore, we will fill in those 3 values with the general median of all the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYWQW9C3xvfR"
      },
      "source": [
        "df_relevant_features['price'].fillna(df_relevant_features['price'].median(), inplace=True)\n",
        "print(df_relevant_features.isnull().sum())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94IJkYBkxvfR"
      },
      "source": [
        "## **Missing Data Imputaion - \"taster_name\" variable**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BBaya1AxvfR"
      },
      "source": [
        "Now we will impute the missing data in the \"taster_name\" variable (character). The missing values will be replaced with the value \"unknown\", since we don't know whether the names are missing due to lack of documintation or deu to the fact that only the major tasters are mentioned in the list. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt6DA2aXxvfS"
      },
      "source": [
        "df_relevant_features['taster_name'].fillna('other', inplace=True)\n",
        "print(df_relevant_features.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4eFrYQOxvfS"
      },
      "source": [
        "## **Missing Data Imputaion - \"region_1\" variable**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKEVvl1axvfS"
      },
      "source": [
        "We only left with the \"region_1\" variable missing data. \n",
        "First, we will check what are countries covered in the \"region_1\" variable - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDqFiUPjxvfS"
      },
      "source": [
        "df_relevant_features.loc[pd.isna(df_relevant_features['region_1']) == False]['country'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvkGxauZxvfT"
      },
      "source": [
        "As we can see, the \"region_1\" feature includes data of only 7 countries out of all 43 countries which appear in the dataset. This means that it is impossible to fill in the missing data of all the other 36 countries. \n",
        "Therefore, the missing values will be replaced with \"null\". "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUm6IByZxvfT"
      },
      "source": [
        "df_relevant_features['region_1'].fillna('other', inplace=True)\n",
        "print(df_relevant_features.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVrpGgp9xvfT"
      },
      "source": [
        "As a summary, after handling the missing data and dropping irrelevant columns, these are the columns we will use for the models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9ahhuvSxvfT"
      },
      "source": [
        "nulls_final = df_relevant_features.isnull().sum()\n",
        "percentage_nulls_final = 100 * nulls_final / len(df_relevant_features)\n",
        "data_types_final = df_relevant_features.dtypes\n",
        "unique_values_final = df_relevant_features.nunique()\n",
        "missing_values_table_final = pd.concat([nulls_final, percentage_nulls_final,unique_values_final, data_types_final], axis=1)\n",
        "missing_values_table_final = missing_values_table_final.rename(columns = {0 : 'Missing Values', 1 : 'Percentage', 2: 'Unique Values', 3 : 'Data Types'})\n",
        "missing_values_table_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j76vnO3xvfU"
      },
      "source": [
        "# **Data Distribution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtHzPXT2xvfU"
      },
      "source": [
        "Before starting working on the models for wine quality prediction, we will check the data distribution of both features and labels. If needed, we will transform the data in order to get a better quality model.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSAlshWVxvfU"
      },
      "source": [
        "### **\"country\" Data Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_U7CoTwxvfU"
      },
      "source": [
        "df_relevant_features['country'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av6WwIwGxvfV"
      },
      "source": [
        "As we see, the big volume of the data is covered by the USA, France, and Italy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isy-dI3nxvfV"
      },
      "source": [
        "### **\"points\" Data Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYAauKtIxvfV"
      },
      "source": [
        "df_relevant_features['points'].hist()\n",
        "plt.suptitle('Histogram of points', fontsize='xx-large')\n",
        "plt.xlabel('Points', fontsize='large')\n",
        "plt.ylabel('Frequency', fontsize='large')\n",
        "print ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOeAQ3IimIK7"
      },
      "source": [
        "print (\"The mean of the points is : {}\".format(df_relevant_features['points'].mean()))\n",
        "print (\"The standard deviation of the points is : {}\".format(df_relevant_features['points'].std()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVdqKLQbxvfV"
      },
      "source": [
        "The scores are normally distributed with mean = 88.5 and standard deviation = 3. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfQDvf6vxvfV"
      },
      "source": [
        "### **\"price\" Data Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j_oMni3xvfV"
      },
      "source": [
        "# An histogram of the \"price\" variable. \n",
        "\n",
        "df_relevant_features['price'].hist()\n",
        "plt.suptitle('Histogram of price', fontsize='xx-large')\n",
        "plt.xlabel('Price is USD', fontsize='large')\n",
        "plt.ylabel('Frequency', fontsize='large')\n",
        "print ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JAPcbMvxvfW"
      },
      "source": [
        "It seems that the outliers in the data prevent are from seeing the distribution of the major amount of data, with much lower prices than the range presented in this histogram. \n",
        "\n",
        "To get a better view, we will first create a boxplot, which will show us the possible outliers in a better way. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEswAtLlxvfW"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.boxplot(y = df_relevant_features['price'])\n",
        "plt.suptitle('Boxplot of price', fontsize='xx-large')\n",
        "print ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQWzEICXxvfW"
      },
      "source": [
        "In the boxplot, we see that most of the prices are lower than ~100. Therefore, we will create a histogram that will provide us a better view of the price distribution. In order to do that, we will summarize all values higher than 100 to one bin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2Ddy6aJxvfW"
      },
      "source": [
        "def plot_histogram():\n",
        "    bins = np.arange(0,110,5)\n",
        "    fig, ax = plt.subplots(figsize=(9, 5))\n",
        "    _, bins, patches = plt.hist([np.clip(df_relevant_features['price'], bins[0], bins[-2])], density=False, bins=bins)\n",
        "    xlabels = bins[0:].astype(str)\n",
        "    xlabels[-2] += '+'\n",
        "    N_labels = len(xlabels)-1\n",
        "    plt.xlim([0, 105])\n",
        "    plt.xticks(5 * np.arange(N_labels))\n",
        "    ax.set_xticklabels(xlabels)\n",
        "    plt.suptitle('Histogram of price', fontsize='xx-large')\n",
        "    plt.xlabel('Price in USD', fontsize='large')\n",
        "    plt.ylabel('Frequency', fontsize='large')\n",
        "    print ()\n",
        "plot_histogram()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD3_a8oWxvfW"
      },
      "source": [
        "As we can see, most of the wines cost between 10-30 USD.\n",
        "\n",
        "Since our data has a right-skewed distribution, we would make a **log transformation**. We expect that the transformation would provide asymptotically normally distributed data.\n",
        "\n",
        "Since the lowest price is 4 and most of the values are below 100, we will present a histogram with a log of 4 to 100 (~1.25 to ~4.75 in log values). All the values higher than 4.75 are summarized and presented in the last bin. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtEv-emnxvfW"
      },
      "source": [
        "def plot_histogram_1():\n",
        "    bins = np.arange(1.25,5.25,0.25)\n",
        "    fig, ax = plt.subplots(figsize=(9, 5))\n",
        "    _, bins, patches = plt.hist([np.clip(df_relevant_features['price'].apply(np.log), bins[0], bins[-2])], density=False, bins=bins)\n",
        "    xlabels = bins[0:].astype(str)\n",
        "    xlabels[-2] += '+'\n",
        "    N_labels = len(xlabels)-1\n",
        "    plt.xlim([1.25,5])\n",
        "    plt.xticks(0.25 * np.arange(N_labels) + 1.25)\n",
        "    ax.set_xticklabels(xlabels)\n",
        "    plt.suptitle('Histogram of log(price)', fontsize='xx-large')\n",
        "    plt.xlabel('log(price)', fontsize='large')\n",
        "    plt.ylabel('Frequency', fontsize='large')\n",
        "    print () \n",
        "plot_histogram_1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM_P01ZqxvfX"
      },
      "source": [
        "The transformed \"price\" variable has indeed a distribution that is much closer to be normal than the original distribution. Hence, we will use log(price) as a feature. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gooC1eqSxvfX"
      },
      "source": [
        "df_relevant_features['price_log'] = np.log(df_relevant_features['price'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LzrF9HZxvfX"
      },
      "source": [
        "### **\"taster_name\" Data Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohOtQ_4IxvfX"
      },
      "source": [
        "df_relevant_features['taster_name'].value_counts().plot(kind='barh')\n",
        "plt.suptitle('Bar chart of tasters', fontsize='xx-large')\n",
        "plt.xlabel('Frequency', fontsize='large')\n",
        "plt.ylabel('Tasters', fontsize='large')\n",
        "print ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMe8FBC9xvfX"
      },
      "source": [
        "We will skip other features since they have a big number of categories each and the summary will not be as informative as the summaries presented above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_lUpc9Kxvfa"
      },
      "source": [
        "\n",
        "data_types_final = df_relevant_features.dtypes\n",
        "unique_values_final = df_relevant_features.nunique()\n",
        "missing_values_table_final = pd.concat([unique_values_final, data_types_final], axis=1)\n",
        "missing_values_table_final = missing_values_table_final.rename(columns = {0 : 'Unique Values', 1 : 'Data Types'})\n",
        "missing_values_table_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2vod1BZSZ5k"
      },
      "source": [
        "In our future models, we will use an One-Hot Encoding for categorical variables. Since this encode will increase the amount of features to be a very high, will slow the running time and use a big amount of memory. \n",
        "In order to reduce all that, categories with a low amount of values (<100 in our case) will be replaced by the value \"other\".  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNxI1EcBgmvt"
      },
      "source": [
        "cols = ['province','region_1','variety', 'winery']\n",
        "for col in cols:\n",
        "    val = df_relevant_features[col].value_counts()\n",
        "    y = val[val < 100].index\n",
        "    df_relevant_features[col] = df_relevant_features[col].replace({x:'other' for x in y})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKvw5fuHhYWm"
      },
      "source": [
        "data_types_final = df_relevant_features.dtypes\n",
        "unique_values_final = df_relevant_features.nunique()\n",
        "missing_values_table_final = pd.concat([unique_values_final, data_types_final], axis=1)\n",
        "missing_values_table_final = missing_values_table_final.rename(columns = {0 : 'Unique Values', 1 : 'Data Types'})\n",
        "missing_values_table_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOW5g2TmX0XE"
      },
      "source": [
        "After replacing the values to \"other\", we will encode the categorical features to One-Hot features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2MUMJgnxvfa"
      },
      "source": [
        "df_relevant_features_new = pd.get_dummies(df_relevant_features, columns = ['country', 'province', 'region_1','taster_name','variety','winery'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g2TSPhmYws5"
      },
      "source": [
        "Since we transrformed the 'price' variable to be log, we will drop the 'price' column. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l80KmJoDxvfb"
      },
      "source": [
        "df_relevant_features_new.drop('price', axis=1, inplace=True)\n",
        "df_relevant_features_new.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8XDiJ75ZGHf"
      },
      "source": [
        "# **Regression Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERJILQbV5vnn"
      },
      "source": [
        "### **Linear Regression Model**\n",
        "\n",
        "As a starting point, we will create a basic Linear Regression Model, which will include only two numeric variables that are included in the dataset - price, and points. We will check the proportion of the variance for the points that is explained by the price. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOD3oYihFuZo"
      },
      "source": [
        "x1 = df_relevant_features_new['price_log'] # independent variable\n",
        "y = df_relevant_features_new['points'] # dependent variable\n",
        "plt.scatter(x1, y)\n",
        "plt.xlabel('log (price)', fontsize = 10)\n",
        "plt.ylabel('points', fontsize = 10)\n",
        "plt.show()\n",
        "\n",
        "x = sm.add_constant(x1)\n",
        "\n",
        "lr_model = sm.OLS(y, x).fit() # Ordinary Least Squares \n",
        "lr_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJuNK_dxLpSB"
      },
      "source": [
        "As we see in the graph and in the summary table, there is some linear relationship between the log of the prices and the scores. \n",
        "The R^2 value is 0.352, means the proportion of the variance for the points that is explained by the price is equal to 0.352. \n",
        "We also see that both the log(price) and the intercept are significant, means the log(price) is a significant variable when predicting the points. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9iDjCs2JsWg"
      },
      "source": [
        "plt.scatter(x1, y)\n",
        "yhat = 2.8480*x1 + 79.008\n",
        "fig = plt.plot(x1,yhat, lw=4, c='orange', label = 'regression line')\n",
        "plt.xlabel('log (price)', fontsize = 10)\n",
        "plt.ylabel('points', fontsize = 10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y1BCnNzM3V7"
      },
      "source": [
        "After creating a basic Linear Regression Model, we will move further to machine learning regression models, and will check how adding categorical features (presented as one-hot features) and using more advanced models will improve the predictions. \n",
        "\n",
        "Later, we will create categorical models, and will try to predict the quality of the wine, when the label will be used as a categorical label with two categories - \"Poor quality\" and \"Good quality\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It63E_bmZYUe"
      },
      "source": [
        "## **Keras Regression Model**\n",
        "\n",
        "We wil start with Neural Network Keras Regression Model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jPrw8LHxvfb"
      },
      "source": [
        "x_columns_reg = df_relevant_features_new.columns.drop('points')\n",
        "x_reg = df_relevant_features_new[x_columns_reg].values\n",
        "y_reg = df_relevant_features_new['points'].values\n",
        "\n",
        "# Create train/test\n",
        "x_train_reg, x_test_reg, y_train_reg, y_test_reg = train_test_split(    \n",
        "    x_reg, y_reg, test_size=0.2, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7hBGuFyxvfc"
      },
      "source": [
        "# Build the neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(25, input_dim=x_reg.shape[1], activation='relu')) # Hidden 1\n",
        "model.add(Dense(10, activation='relu')) # Hidden 2\n",
        "model.add(Dense(1)) # Output\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "                        patience=5, verbose=1, mode='auto', \n",
        "                        restore_best_weights=True)\n",
        "model.fit(x_train_reg,y_train_reg,validation_data=(x_test_reg,y_test_reg),\n",
        "          callbacks=[monitor],verbose=2,epochs=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzKtHj3Cxvfc"
      },
      "source": [
        "# Predict\n",
        "y_pred_keras = model.predict(x_test_reg)\n",
        "\n",
        "# Measure MSE and RMSE error.  \n",
        "\n",
        "score1 = metrics.mean_squared_error(y_pred_keras,y_test_reg)\n",
        "print(\"Final score (MSE): {}\".format(score1))\n",
        "score2 = np.sqrt(metrics.mean_squared_error(y_pred_keras,y_test_reg))\n",
        "print(\"Final score (RMSE): {}\".format(score2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKlkvhBdpw-B"
      },
      "source": [
        "# Calculating R2 value\n",
        "\n",
        "y_pred_train_keras = model.predict(x_train_reg)\n",
        "\n",
        "score_train_keras = r2_score(y_train_reg, y_pred_train_keras)\n",
        "score_test_keras = r2_score(y_test_reg, y_pred_keras)\n",
        "print(\"R^2 for train data: {}\".format(score_train_keras))\n",
        "print(\"R^2 for test data: {}\".format(score_test_keras))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di_WihTHU5WW"
      },
      "source": [
        "# Regression chart.\n",
        "def chart_regression(pred, y, sort=True):\n",
        "    t = pd.DataFrame({'pred': pred, 'y': y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'], inplace=True)\n",
        "    plt.plot(t['pred'].tolist(), label='prediction')\n",
        "    plt.plot(t['y'].tolist(), label='expected')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "chart_regression(y_pred_keras.flatten(), y_test_reg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90Ssl7eusgtU"
      },
      "source": [
        "As we see, the RMSE value is 2.28 and the R^2 value for the test data is 0.45. \n",
        "\n",
        "We will build aditional Regression model and compare the scores.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdY7z5hCtGMw"
      },
      "source": [
        "## **XGBoost Regression Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZQqbqqGEtiB"
      },
      "source": [
        "regressor = xgb.XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    reg_lambda=2,\n",
        "    gamma=0,\n",
        "    max_depth=3\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGgJeLuIKBSd"
      },
      "source": [
        "regressor.fit(x_train_reg, y_train_reg)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obPAg3cqKFju"
      },
      "source": [
        "y_pred_XG = regressor.predict(x_test_reg)\n",
        "\n",
        "# Measure MSE and RMSE errors  \n",
        "\n",
        "score1_XG = metrics.mean_squared_error(y_pred_XG,y_test_reg)\n",
        "print(\"Final score (MSE): {}\".format(score1_XG))\n",
        "score2_XG = np.sqrt(metrics.mean_squared_error(y_pred_XG,y_test_reg))\n",
        "print(\"Final score (RMSE): {}\".format(score2_XG))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBF8RMrsRoB5"
      },
      "source": [
        "# Calculating R2 value\n",
        "\n",
        "y_pred_train_XG = regressor.predict(x_train_reg)\n",
        "score_train_XG = r2_score(y_train_reg,y_pred_train_XG)\n",
        "score_test_XG = r2_score(y_test_reg,y_pred_XG)\n",
        "print(\"R^2 for train data: {}\".format(score_train_XG))\n",
        "print(\"R^2 for test data: {}\".format(score_test_XG))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaXTSjEUbh9c"
      },
      "source": [
        "# Regression chart.\n",
        "def chart_regression(pred, y, sort=True):\n",
        "    t = pd.DataFrame({'pred': pred, 'y': y.flatten()})\n",
        "    if sort:\n",
        "        t.sort_values(by=['y'], inplace=True)\n",
        "    plt.plot(t['pred'].tolist(), label='prediction')\n",
        "    plt.plot(t['y'].tolist(), label='expected')\n",
        "    plt.ylabel('output')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "chart_regression(y_pred_XG.flatten(), y_test_reg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuWDrIjsxAQm"
      },
      "source": [
        "XDBoost Regression model provides a bit lower scores than the Keras Regression model. Both scores are not really good and do not provide a very good prediction for the quality of the wine. \n",
        "In addition, in the Regression chart we see that the predcition in XGBoost model have less variance than the predictions in Keras Model and almost don`t show any trend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3F46uJRxnH_"
      },
      "source": [
        "# **Binary Classification Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDr5RlBpx31R"
      },
      "source": [
        "After predicting the scores of the wines using models with  a numeric label, we will encode our label to be a two-categories label (poor/good quality wine). Following that, the models shown below will be binary classification models.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvRcLD7ijd7Q"
      },
      "source": [
        "df_relevant_features_new['points_cat'] = np.where(df_relevant_features_new[\"points\"]>=89, 1, 2) \n",
        "df_relevant_features_new['points_cat'].value_counts()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPPelUMcEz6Q"
      },
      "source": [
        "df_relevant_features_new.drop('points', axis=1, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASYOYXwfGaNk"
      },
      "source": [
        "x_columns_class = df_relevant_features_new.columns.drop('points_cat')\n",
        "x_class = df_relevant_features_new[x_columns_class].values\n",
        "y_class = df_relevant_features_new['points_cat'].values\n",
        "\n",
        "# Create train/test\n",
        "x_train_class, x_test_class, y_train_class, y_test_class = train_test_split(    \n",
        "    x_class, y_class, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mYYkz87zxjd"
      },
      "source": [
        "## **Naive Bayes Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sEFUCDiL2sl"
      },
      "source": [
        "#Create a Gaussian Classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "#Train the model using the training sets\n",
        "gnb.fit(x_train_class, y_train_class)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred_NB = gnb.predict(x_test_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHJwr-jUjDB0"
      },
      "source": [
        "# Confusion matrix \n",
        "\n",
        "def pretty_print_conf_matrix(y_true, y_pred, \n",
        "                             classes,\n",
        "                             normalize=False,\n",
        "                             title='Confusion matrix',\n",
        "                             cmap=plt.cm.Pastel2):\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Configure Confusion Matrix Plot Aesthetics (no text yet) \n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title, fontsize=14)\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=80)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    plt.ylabel('True label', fontsize=12)\n",
        "    plt.xlabel('Predicted label', fontsize=12)\n",
        "    # Calculate normalized values (so all cells sum to 1) if desired\n",
        "    if normalize:\n",
        "        cm = np.round(cm.astype('float') / cm.sum(),2) #(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # Place Numbers as Text on Confusion Matrix Plot\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\",\n",
        "                 fontsize=12)\n",
        "\n",
        "\n",
        "    # Add Precision, Recall, F-1 Score as Captions Below Plot\n",
        "    rpt = classification_report(y_true, y_pred)\n",
        "    rpt = rpt.replace('avg / total', '      avg')\n",
        "    rpt = rpt.replace('support', 'N Obs')\n",
        "\n",
        "    plt.annotate(rpt, \n",
        "                 xy = (0,0), \n",
        "                 xytext = (-20, -200), \n",
        "                 xycoords='axes fraction', textcoords='offset points',\n",
        "                 fontsize=12, ha='left')    \n",
        "\n",
        "    # Plot\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.style.use('classic')\n",
        "plt.figure(figsize=(2,2))\n",
        "pretty_print_conf_matrix(y_test_class, y_pred_NB, \n",
        "                         classes= ['Poor=1', 'Good=2'],\n",
        "                         normalize=True, \n",
        "                         title='Confusion Matrix')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsEYYGNy0aCa"
      },
      "source": [
        "The confusion matrix showed us that the accuracy value is 0.64, which means the models classified correctly only 64% of the data. \n",
        "In order to try and improve the model, we will run the same model, but this time we will change to price feature to be a categorical feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUv2O5r8Nacu"
      },
      "source": [
        "bins = [0, 2.75, 3, 3.5, 4, np.inf]\n",
        "names = ['1', '2', '3', '4', '5']\n",
        "\n",
        "df_relevant_features_new['price_log_cat'] = pd.cut(df_relevant_features_new['price_log'], bins, labels=names)\n",
        "df_relevant_features_new['price_log_cat'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ly5_mG_uohBx"
      },
      "source": [
        "x_columns_class_all_cat = df_relevant_features_new.columns.drop('points_cat').drop('price_log')\n",
        "x_class_all_cat = df_relevant_features_new[x_columns_class_all_cat].values\n",
        "y_class_all_cat = df_relevant_features_new['points_cat'].values\n",
        "\n",
        "# Create train/test\n",
        "\n",
        "x_train_class_all_cat, x_test_class_all_cat, y_train_class_all_cat, y_test_class_all_cat = train_test_split(    \n",
        "    x_class_all_cat, y_class_all_cat, test_size=0.2, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Vl7BllrpG_"
      },
      "source": [
        "#Create a Gaussian Classifier\n",
        "gnb_cat = GaussianNB()\n",
        "\n",
        "#Train the model using the training sets\n",
        "gnb_cat.fit(x_train_class_all_cat, y_train_class_all_cat)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred_NB_all_cat = gnb_cat.predict(x_test_class_all_cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vuNDH36oObG"
      },
      "source": [
        "# Confusion matrix \n",
        "\n",
        "plt.style.use('classic')\n",
        "plt.figure(figsize=(2,2))\n",
        "pretty_print_conf_matrix(y_test_class, y_pred_NB_all_cat, \n",
        "                         classes= ['Poor=1', 'Good=2'],\n",
        "                         normalize=True, \n",
        "                         title='Confusion Matrix')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ9lEzaA1cpN"
      },
      "source": [
        "As we see, once we changed the price feature to be categorical, the accuracy hasn`t chanched.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZDRjLvr109Y"
      },
      "source": [
        "## **XGBoost Classification Model**\n",
        "\n",
        "\n",
        "After seeing a XGBoost Regression Model, we will build a XGBoost Classification Model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHoUvsT_sDHg"
      },
      "source": [
        "xgbc = xgb.XGBClassifier(n_estimators=100)\n",
        "xgbc.fit(x_train_class, y_train_class)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSO3RNPrqbtp"
      },
      "source": [
        "#Predict the response for test dataset\n",
        "y_pred_XGBclass = xgbc.predict(x_test_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8icNQw-tgBF"
      },
      "source": [
        "# Confusion matrix \n",
        "\n",
        "plt.style.use('classic')\n",
        "plt.figure(figsize=(2,2))\n",
        "pretty_print_conf_matrix(y_test_class, y_pred_XGBclass, \n",
        "                         classes= ['Poor=1', 'Good=2'],\n",
        "                         normalize=True, \n",
        "                         title='Confusion Matrix')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f36dmOQ7AqiG"
      },
      "source": [
        "Seems that the currunt XGBoost model is the one that provides the highest percentage of accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_olrFA7EBEQJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}